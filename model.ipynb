{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "17a737e2-ff67-6170-1229-bb95dbb2f65a"
   },
   "source": [
    "# Start-to-Finish Solution in Keras\n",
    "\n",
    "Here is my basic method for getting a LB submission churned out. No parameter tuning or data augmentation has been attempted, which should increase the score significantly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "e6a4aad9-f09f-8f20-a9d0-159f5ef6c922",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 1080 (CNMeM is disabled, cuDNN 5105)\n",
      "/home/rishikesh/.local/lib/python3.5/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import os, random\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Convolution2D, MaxPooling2D, ZeroPadding2D, Dense, Activation\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "\n",
    "TRAIN_DIR = '../train/'\n",
    "TEST_DIR = '../test_stg1/'\n",
    "FISH_CLASSES = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "ROWS = 90  #720\n",
    "COLS = 160 #1280\n",
    "CHANNELS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dec4ed7b-f0b8-0c30-5431-3bd652c190da"
   },
   "source": [
    "# Loading and Preprocessing Data\n",
    "\n",
    "Not much processing, other than resizing to 360x640, but you will probably want to run larger images on a GPU for a higher score. I am also keeping track of the labels as I loop through each image folder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "d8f4e77d-ce91-48f7-c04a-ba225f893f0c",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719 photos of ALB\n",
      "200 photos of BET\n",
      "117 photos of DOL\n",
      "67 photos of LAG\n",
      "465 photos of NoF\n",
      "299 photos of OTHER\n",
      "176 photos of SHARK\n",
      "734 photos of YFT\n"
     ]
    }
   ],
   "source": [
    "def get_images(fish):\n",
    "    \"\"\"Load files from train folder\"\"\"\n",
    "    fish_dir = TRAIN_DIR+'{}'.format(fish)\n",
    "    images = [fish+'/'+im for im in os.listdir(fish_dir)]\n",
    "    return images\n",
    "\n",
    "def read_image(src):\n",
    "    \"\"\"Read and resize individual images\"\"\"\n",
    "    im = io.imread(src)\n",
    "    im = resize(im, (ROWS,COLS), order=3)\n",
    "    return im\n",
    "\n",
    "\n",
    "files = []\n",
    "y_all = []\n",
    "\n",
    "for fish in FISH_CLASSES:\n",
    "    fish_files = get_images(fish)\n",
    "    files.extend(fish_files)\n",
    "    \n",
    "    y_fish = np.tile(fish, len(fish_files))\n",
    "    y_all.extend(y_fish)\n",
    "    print(\"{0} photos of {1}\".format(len(fish_files), fish))\n",
    "    \n",
    "y_all = np.array(y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "7547d288-1ca9-c45e-0efb-75ea832cb318",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 of 3777\n",
      "Processed 1000 of 3777\n",
      "Processed 2000 of 3777\n",
      "Processed 3000 of 3777\n",
      "(3777, 90, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "X_all = np.ndarray((len(files), ROWS, COLS, CHANNELS), dtype=np.uint8)\n",
    "\n",
    "for i, im in enumerate(files): \n",
    "    X_all[i] = read_image(TRAIN_DIR+im)\n",
    "    if i%1000 == 0: print('Processed {} of {}'.format(i, len(files)))\n",
    "\n",
    "print(X_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "18a6f374-8c0e-b332-3e46-46a732b96930",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Uncomment to check out a fish from each class\n",
    "#uniq = np.unique(y_all, return_index=True)\n",
    "# for f, i in zip(uniq[0], uniq[1]):\n",
    "    #plt.imshow(X_all[i])\n",
    "    #plt.title(f)\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "25a3b3b2-a00e-d618-9d2c-4dc18ad14744"
   },
   "source": [
    "# Splitting the Training Data\n",
    "\n",
    "One-Hot-Encode the labels, then create a stratified train/validation split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "0537e039-ae27-ea4e-82a6-23603d9aa007",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# One Hot Encoding Labels\n",
    "y_all = LabelEncoder().fit_transform(y_all)\n",
    "y_all = np_utils.to_categorical(y_all)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_all, y_all, \n",
    "                                                    test_size=0.2, random_state=23, \n",
    "                                                    stratify=y_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e2256352-79ff-78eb-24d8-f1874d32cbf4"
   },
   "source": [
    "## The Model\n",
    "\n",
    "Pretty typical CNN in Keras with a plenty of dropout regularization between the fully connected layers. Note: I set the epochs to 1 to avoid timing out - change it to around 20. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "6e2396d3-d295-9616-d327-99cbe76b8851",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=1e-4)\n",
    "objective = 'categorical_crossentropy'\n",
    "\n",
    "def center_normalize(x):\n",
    "    return (x - K.mean(x)) / K.std(x)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Activation(activation=center_normalize, input_shape=(ROWS, COLS, CHANNELS)))\n",
    "\n",
    "model.add(Convolution2D(32, 5, 5, border_mode='same', activation='relu', dim_ordering='tf'))\n",
    "model.add(Convolution2D(32, 5, 5, border_mode='same', activation='relu', dim_ordering='tf'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), dim_ordering='tf'))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same', activation='relu', dim_ordering='tf'))\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same', activation='relu', dim_ordering='tf'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), dim_ordering='tf'))\n",
    "\n",
    "model.add(Convolution2D(128, 3, 3, border_mode='same', activation='relu', dim_ordering='tf'))\n",
    "model.add(Convolution2D(128, 3, 3, border_mode='same', activation='relu', dim_ordering='tf'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), dim_ordering='tf'))\n",
    "\n",
    "model.add(Convolution2D(256, 3, 3, border_mode='same', activation='relu', dim_ordering='tf'))\n",
    "model.add(Convolution2D(256, 3, 3, border_mode='same', activation='relu', dim_ordering='tf'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), dim_ordering='tf'))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(len(FISH_CLASSES)))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss=objective, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "461ff410-5bca-f26d-5b4b-722fb2681e33",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2416 samples, validate on 605 samples\n",
      "Epoch 1/20\n",
      "2416/2416 [==============================] - 13s - loss: 1.9161 - val_loss: 1.7788\n",
      "Epoch 2/20\n",
      "2416/2416 [==============================] - 13s - loss: 1.8306 - val_loss: 1.8198\n",
      "Epoch 3/20\n",
      "2416/2416 [==============================] - 13s - loss: 1.8157 - val_loss: 1.7852\n",
      "Epoch 4/20\n",
      "2416/2416 [==============================] - 13s - loss: 1.8056 - val_loss: 1.7898\n",
      "Epoch 5/20\n",
      "2416/2416 [==============================] - 13s - loss: 1.8066 - val_loss: 1.7585\n",
      "Epoch 6/20\n",
      "2416/2416 [==============================] - 13s - loss: 1.7823 - val_loss: 1.6969\n",
      "Epoch 7/20\n",
      "2416/2416 [==============================] - 13s - loss: 1.7863 - val_loss: 1.6607\n",
      "Epoch 8/20\n",
      "2416/2416 [==============================] - 13s - loss: 1.7519 - val_loss: 1.6331\n",
      "Epoch 9/20\n",
      "2416/2416 [==============================] - 13s - loss: 1.7253 - val_loss: 1.6897\n",
      "Epoch 10/20\n",
      "2416/2416 [==============================] - 13s - loss: 1.7064 - val_loss: 1.6151\n",
      "Epoch 11/20\n",
      "2416/2416 [==============================] - 13s - loss: 1.6800 - val_loss: 1.5549\n",
      "Epoch 12/20\n",
      "2416/2416 [==============================] - 13s - loss: 1.6476 - val_loss: 1.5423\n",
      "Epoch 13/20\n",
      "2416/2416 [==============================] - 13s - loss: 1.6382 - val_loss: 1.4878\n",
      "Epoch 14/20\n",
      "2416/2416 [==============================] - 13s - loss: 1.5783 - val_loss: 1.5160\n",
      "Epoch 15/20\n",
      "2416/2416 [==============================] - 13s - loss: 1.5825 - val_loss: 1.3942\n",
      "Epoch 16/20\n",
      "2416/2416 [==============================] - 13s - loss: 1.5421 - val_loss: 1.3291\n",
      "Epoch 17/20\n",
      "2416/2416 [==============================] - 13s - loss: 1.5186 - val_loss: 1.3314\n",
      "Epoch 18/20\n",
      "2416/2416 [==============================] - 13s - loss: 1.4637 - val_loss: 1.3972\n",
      "Epoch 19/20\n",
      "2416/2416 [==============================] - 13s - loss: 1.4397 - val_loss: 1.3341\n",
      "Epoch 20/20\n",
      "2416/2416 [==============================] - 13s - loss: 1.4054 - val_loss: 1.2882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7cef343c50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=1, mode='auto')        \n",
    "        \n",
    "model.fit(X_train, y_train, batch_size=64, nb_epoch=20,\n",
    "              validation_split=0.2, verbose=1, shuffle=True, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "6c3c125c-e273-99a5-8360-8b717b9b9e58",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756/756 [==============================] - 0s     \n",
      "Validation Log Loss: 1.2596293551029352\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X_valid, verbose=1)\n",
    "print(\"Validation Log Loss: {}\".format(log_loss(y_valid, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6020ea37-c78f-a6f3-0877-2196833f8790"
   },
   "source": [
    "# Predicting the Test Set\n",
    "\n",
    "Finishing off with predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "c4769ce7-367c-5240-b615-714972d8e596",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 992/1000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "test_files = [im for im in os.listdir(TEST_DIR)]\n",
    "test = np.ndarray((len(test_files), ROWS, COLS, CHANNELS), dtype=np.uint8)\n",
    "\n",
    "for i, im in enumerate(test_files): \n",
    "    test[i] = read_image(TEST_DIR+im)\n",
    "    \n",
    "test_preds = model.predict(test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "dfe3630b-9944-e91e-102d-d26efdcaa90f",
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>ALB</th>\n",
       "      <th>BET</th>\n",
       "      <th>DOL</th>\n",
       "      <th>LAG</th>\n",
       "      <th>NoF</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>SHARK</th>\n",
       "      <th>YFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img_00170.jpg</td>\n",
       "      <td>0.884204</td>\n",
       "      <td>0.203362</td>\n",
       "      <td>0.017741</td>\n",
       "      <td>0.588164</td>\n",
       "      <td>0.238697</td>\n",
       "      <td>0.039009</td>\n",
       "      <td>0.037523</td>\n",
       "      <td>0.528543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img_05927.jpg</td>\n",
       "      <td>0.886872</td>\n",
       "      <td>0.312247</td>\n",
       "      <td>0.168433</td>\n",
       "      <td>0.128648</td>\n",
       "      <td>0.669172</td>\n",
       "      <td>0.224466</td>\n",
       "      <td>0.177974</td>\n",
       "      <td>0.361756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img_06860.jpg</td>\n",
       "      <td>0.886872</td>\n",
       "      <td>0.312247</td>\n",
       "      <td>0.168433</td>\n",
       "      <td>0.128648</td>\n",
       "      <td>0.669172</td>\n",
       "      <td>0.224466</td>\n",
       "      <td>0.177974</td>\n",
       "      <td>0.361756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img_02734.jpg</td>\n",
       "      <td>0.924187</td>\n",
       "      <td>0.172862</td>\n",
       "      <td>0.016748</td>\n",
       "      <td>0.657391</td>\n",
       "      <td>0.405113</td>\n",
       "      <td>0.066585</td>\n",
       "      <td>0.034425</td>\n",
       "      <td>0.194739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img_00152.jpg</td>\n",
       "      <td>0.957233</td>\n",
       "      <td>0.222972</td>\n",
       "      <td>0.020450</td>\n",
       "      <td>0.169733</td>\n",
       "      <td>0.025794</td>\n",
       "      <td>0.096475</td>\n",
       "      <td>0.139495</td>\n",
       "      <td>0.921968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           image       ALB       BET       DOL       LAG       NoF     OTHER  \\\n",
       "0  img_00170.jpg  0.884204  0.203362  0.017741  0.588164  0.238697  0.039009   \n",
       "1  img_05927.jpg  0.886872  0.312247  0.168433  0.128648  0.669172  0.224466   \n",
       "2  img_06860.jpg  0.886872  0.312247  0.168433  0.128648  0.669172  0.224466   \n",
       "3  img_02734.jpg  0.924187  0.172862  0.016748  0.657391  0.405113  0.066585   \n",
       "4  img_00152.jpg  0.957233  0.222972  0.020450  0.169733  0.025794  0.096475   \n",
       "\n",
       "      SHARK       YFT  \n",
       "0  0.037523  0.528543  \n",
       "1  0.177974  0.361756  \n",
       "2  0.177974  0.361756  \n",
       "3  0.034425  0.194739  \n",
       "4  0.139495  0.921968  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame(test_preds, columns=FISH_CLASSES)\n",
    "submission.insert(0, 'image', test_files)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_file = 'submission_4.csv'\n",
    "submission.to_csv(sub_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 9,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
